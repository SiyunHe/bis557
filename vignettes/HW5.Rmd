---
title: "HW5"
author: "Siyun He"
date: "12/19/2018"
output: html_document
---
##Number1

```{r,eval=FALSE}
#Reference: http://apapiu.github.io/2016-01-02-minst/
#load packages
library(tidyverse)
library(glmnet)
library(tidyr)
library(ggplot2)
library(readr)
library(tensorflow)
library(moments)
library(keras)

mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

#Reshape 
x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)
```

Model Performance (without intensity in features)
```{r,eval=FALSE}
set.seed(666)
sample <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[sample,], y_train[sample], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
#Test dataset prediction accuracy: 0.8452

#Exploration 
intensity<-apply(x_train,1,mean) #takes the mean of each row in train
intbylabel<-aggregate(intensity,by=list(y_train),FUN = mean)
plot<-ggplot(data =intbylabel,aes(x=Group.1,y=x))+geom_bar(stat = "identity",fill ="blue")
plot+scale_x_discrete(limits=0:9)+xlab("Digit label")+ylab("Average intensity")
#According to the plot, it can be seen that the means are dfferent

kurtosis<-apply(x_train,1,kurtosis)
klabel<-aggregate(kurtosis,by=list(y_train),FUN = kurtosis)
plot<-ggplot(data = klabel,aes(x=Group.1,y=x))+geom_bar(stat = "identity", fill = "green")
plot+scale_x_discrete(limits=0:9)+xlab("Digit label")+ylab("Kurtosis")
#According to the plot, it can be seen that the kurtosis are different

skewness<-apply(x_train,1,skewness)
slabel<-aggregate(skewness,by=list(y_train),FUN = skewness)
plot<-ggplot(data = slabel,aes(x=Group.1,y=x))+geom_bar(stat = "identity", fill = "red")
plot+scale_x_discrete(limits=0:9)+xlab("digit label")+ylab("skewness")
#According to the plot, it can be seen that the skewness are different

intensity<-as.vector(intensity)
x_train<-cbind(x_train,intensity)
x_test<-cbind(x_test,as.vector(apply(x_test,1,mean)))
```

Model Performance (with intensity in the features)

```{r,eval=FALSE}
set.seed(666)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
#Test dataset prediction accuracy: 0.8483
```

Model Performance (with both intensity and kurtosis in the features)

```{r,eval=FALSE}
set.seed(666)
kurtosis<-as.vector(kurtosis)
x_train<-cbind(x_train,kurtosis)
x_test<-cbind(x_test,as.vector(apply(x_test,1,kurtosis)))
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")
preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda.min, 
                 type = "class")
t <- table(as.vector(preds), y_test)
sum(diag(t)) / sum(t)
#Test dataset prediction accuracy: 0.8483
```

According to these three results, we can see that both models with intensity and intensity + kurtosis in the model are better the first model

```{r,eval=FALSE}
#Reference:https://keras.rstudio.com/articles/examples/mnist_cnn.html
# library(keras)
#library(keras)
#install_keras()

#Define parameters
batch_size <- 128
num_classes <- 10
epochs <- 2
# Input image dimensions
img_rows <- 28
img_cols <- 28
# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# Redefine  dimension of train/test inputs
x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define model
model <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
                input_shape = input_shape) %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
  loss = loss_categorical_crossentropy,
  optimizer = optimizer_adadelta(),
  metrics = c('accuracy')
)
# Train model
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = epochs,
  validation_split = 0.2
)
scores <- model %>% evaluate(
  x_test, y_test, verbose = 0
)
# Output metrics
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')
```
According to the results above we can see that the CNN has a better ability in identifying handwriting characters than LASSO.


##Number 2
```{r}
# install.packages("R.matlab")
library(R.matlab)
emnist <- readMat("/Users/siyunhe/Desktop/HW5/matlab/emnist-byclass.mat")
save(emnist, file='/Users/siyunhe/Desktop/HW5/emnist.RData')

load("/Users/siyunhe/Desktop/HW5/emnist.RData")

# train data
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
  x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, num_classes = 26)
# test set
X_test <- emnist$dataset[[2]][[1]]/255
x_test <- array(dim = c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
  x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
y_test <- as.vector(emnist$dataset[[2]][[2]])-1
y_test <- to_categorical(y_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

library(keras)

model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                input_shape = c(28, 28, 1),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 32, kernel_size = c(5,5),
                padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(5,5)) %>%
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")
# Compile model
model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))
# Train model
model %>% fit(
  x_train, y_train,
  epochs = 10,
  validation_data = list(x_test,y_test)
)
predict.train <- predict_classes(model, x_train)
predict.test <- predict_classes(model, x_test)
# Output metrics
cat('Train:', predict.train==(emnist_train[,1]-1), '\n')
#0.9235239
cat('Test:', predict.test==(emnist_test[,1]-1), '\n')
#0.9029663
```
```
##Number 3
```{r}
#According to the textbook page 210
# Create list of weights to describe a dense neural network.
##Args:
# sizes: A vector giving the size of each layer, including
# the input and output layers.
##Returns:
# A list containing initialized weights and biases.
casl_nn_make_weights <-
function(sizes)
{
L <- length(sizes) - 1L
weights <- vector("list", L)
for (j in seq_len(L))
{
w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),
ncol = sizes[j],
nrow = sizes[j + 1L])
weights[[j]] <- list(w=w,
b=rnorm(sizes[j + 1L]))
}
weights
}

# Apply a rectified linear unit (ReLU) to a vector/matrix.
##Args:
# v: A numeric vector or matrix.
##Returns:
casl_util_ReLU <-
function(v)
{
v[v < 0] <- 0
v
}

# Apply derivative of the rectified linear unit (ReLU).
##Args:
# v: A numeric vector or matrix.
##Returns:
# Sets positive values to 1 and negative values to zero.
casl_util_ReLU_p <-
function(v)
{
p <- v * 0
p[v > 0] <- 1
p
}

casl_util_mad_p <- function(y, a) {
  derloss <- c()
  for (i in 1:length(a)) {
    if (a[i] >= mean(y)) derloss[i]=1
    else derloss[i]=-1
  }
  return(derloss)
}


# Apply forward propagation to a set of NN weights and biases.
##Args:
# x: A numeric vector representing one row of the input.
# weights: A list created by casl_nn_make_weights.
# sigma: The activation function.
##Returns:
# A list containing the new weighted responses (z) and
# activations (a).
casl_nn_forward_prop <-
function(x, weights, sigma)
{
L <- length(weights)
z <- vector("list", L)
a <- vector("list", L)
for (j in seq_len(L))
{
a_j1 <- if(j == 1) x else a[[j - 1L]]
z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
}
list(z=z, a=a)
}

# Apply backward propagation algorithm.
##Args:
# x: A numeric vector representing one row of the input.
# y: A numeric vector representing one row of the response.
# weights: A list created by casl_nn_make_weights.
# f_obj: Output of the function casl_nn_forward_prop.
# sigma_p: Derivative of the activation function.
# f_p: Derivative of the loss function.
##Returns:
# A list containing the new weighted responses (z) and
# activations (a).
casl_nn_backward_prop <-function(x, y, weights, f_obj, sigma_p, f_p)
{
z <- f_obj$z; a <- f_obj$a
L <- length(weights)
grad_z <- vector("list", L)
grad_w <- vector("list", L)
for (j in rev(seq_len(L)))
{
if (j == L)
{
grad_z[[j]] <- f_p(y, a[[j]])
} else {
grad_z[[j]] <- (t(weights[[j + 1]]$w) %*%
grad_z[[j + 1]]) * sigma_p(z[[j]])
}
a_j1 <- if(j == 1) x else a[[j - 1L]]
grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
}
list(grad_z=grad_z, grad_w=grad_w)
}

# Apply stochastic gradient descent (SGD) to estimate NN.
##Args:
# X: A numeric data matrix.
# y: A numeric vector of responses.
# sizes: A numeric vector giving the sizes of layers in
# the neural network.
# epochs: Integer number of epochs to computer.
# eta: Positive numeric learning rate.
# weights: Optional list of starting weights.
##Returns:
# A list containing the trained weights for the network.
casl_nn_sgd <- function(X, y, sizes, epochs, eta, weights=NULL) {
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
    }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights, f_obj, casl_util_ReLU_p, casl_util_mad_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    } 
  }
  weights 
}

# Predict values from a training neural network.
##Args:
# weights: List of weights describing the neural network.
# X_test: A numeric data matrix for the predictions.
##Returns:
# A matrix of predicted values.
casl_nn_predict <-
function(weights, X_test)
{
  p <- length(weights[[length(weights)]]$b)
y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
for (i in seq_len(nrow(X_test)))
{
a <- casl_nn_forward_prop(X_test[i,], weights,
casl_util_ReLU)$a
y_hat[i, ] <- a[[length(a)]]
}
y_hat
}
```

#Simulations:

```{r, echo=FALSE}
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
yn <- X[,1,drop = FALSE]^ 2 + rnorm(1000, sd = 0.1)
ind <- sample(seq_along(yn), 100)
yn[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))
weights = casl_nn_sgd(X, yn, sizes = c(1, 25, 1), epochs=10, eta=0.001)
y_pred = casl_nn_predict(weights, X)
## visualiza the true value and predicted value
df = tibble(x = as.vector(X), y_pred = as.vector(y_pred),
             y = X[,1]^2, yn = as.vector(yn))
ggplot(df) + 
  geom_point(aes(x = x, y = yn)) +
  geom_line(aes(x = x, y = y_pred), color="blue") +
  labs(x = "x", y = "y (True vs. Predict)") + 
  ggtitle("Ture vs. Predicted Values with MSE")
```
According to the plot, the predicted values and the predicted values are close. Neural network with mean absolute deviation as a loss function is robust.






